# ğŸš€ LegalAgentBench
[![GitHub Sponsors](https://img.shields.io/badge/sponsors-GitHub-blue?logo=github&logoColor=white)](https://github.com/sponsors) ![Build Status](https://img.shields.io/badge/build-passing-brightgreen) ![License](https://img.shields.io/badge/license-MIT-yellow) ![Contributors](https://img.shields.io/badge/contributors-10-yellow) ![Awesome List](https://img.shields.io/badge/awesome-awesome-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red)

# ğŸŒŸ About This Repo

This repo is for our paper: 

ğŸ“ LegalAgentBench: Evaluating LLM Agents in Legal Domain.

With the increasing intelligence and autonomy of LLM agents, their potential applications in the legal domain are becoming increasingly apparent. However, existing general-domain benchmarks cannot fully capture the complexity and subtle nuances of real-world judicial cognition and decision-making. 

Therefore, we propose **LegalAgentBench**, a comprehensive benchmark specifically designed to evaluate LLM Agents in the Chinese legal domain. 

LegalAgentBench includes 17 corpora from real-world legal scenarios and provides 37 tools for interacting with external knowledge. We designed a scalable task construction framework and carefully annotated 300 tasks. These tasks span various types, including multi-hop reasoning and writing, and range across different difficulty levels, effectively reflecting the complexity of real-world legal scenarios. 

## ğŸ§© Characteristic

- **Focus on Authentic Legal Scenarios:** LegalAgentBench is the first dataset to evaluate LLM agents in legal scenarios. It requires LLMs to demonstrate a solid understanding of legal principles, enabling them to appropriately select and utilize tools to solve complex legal problems. This represents a significant step forward in advancing the application of LLM agents in legal scenarios.

- **Diverse Task Types and Difficulty Levels:** LegalAgentBench adopts a scalable task construction framework aimed at comprehensively covering various task types and difficulty levels. Specifically, we construct a planning tree based on the dependencies between the corpus and tools, and select tasks through hierarchical sampling and a maximum coverage strategy. Finally, we constructed 300 distinct tasks, including multi-hop reasoning and writing tasks, to comprehensively evaluate the LLMâ€™s capabilitiesã€‚

- **Fine-Grained Evaluation Metrics:** Rather than relying solely on final success rates as evaluation criteria, LegalAgentBench introduces the process rate through the annotation of intermediate steps, enabling fine-grained evaluation. This approach provides deeper insights into an agentâ€™s capabilities and identifies areas for improvement beyond the final result.

## ğŸŒ³ Repo Structure
```
LegalAgentBench/
â”‚
â”œâ”€â”€ data/               
|   |â”€â”€ dataset.json            # Question and Answer Set
â”œâ”€â”€ src/
|   |â”€â”€ evaluation/             # evlaution example
|   |â”€â”€ output/                 # output example
|   |â”€â”€ token/                  # token consumption records
|   â”œâ”€â”€ generated_tools.py      # tools can be used for LLM Agents
|   â”œâ”€â”€ globals.py              # global variables
|   â”œâ”€â”€ plan_and_excute.py      # code for plan_and_excute method
|   â”œâ”€â”€ plan_and_solve.py       # code for plan_and_solve method
|   â”œâ”€â”€ react.py                # code for react method              
|   â”œâ”€â”€ schema.py               # definition for corpus
|   â”œâ”€â”€ prompt.py 
|   â””â”€â”€ utils.py                
â”œâ”€â”€ agents.py                   # definition for agents
â”œâ”€â”€ fewshots.py                 # fewshots for agents
â”œâ”€â”€ prompts.py                  # prompts for agents
```

## âš™ï¸ Quick Start
```python
git clone https://github.com/CSHaitao/LegalAgentBench.git
cd LegalAgentBench
pip install -r requirements.txt

cd src
python react.py
```
â—ï¸ Important: Replace the string your_api_key in utils.py with the actual key.